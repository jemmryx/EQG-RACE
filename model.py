import timeimport configimport torchimport torch.nn as nnimport torch.nn.functional as Ffrom data_utils import UNK_IDfrom gcn import GCNfrom torch import FloatTensorfrom torch.autograd import Variablefrom torch.nn.functional import sigmoid, softmaxfrom torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequencefrom torch_scatter import scatter_maxINF = 1e12class Encoder(nn.Module):    def __init__(self, embeddings, vocab_size, embedding_size, hidden_size, num_layers, dropout):        super(Encoder, self).__init__()        self.embedding = nn.Embedding(vocab_size, embedding_size)        if config.use_tag:            self.tag_embedding = nn.Embedding(4, 32)            lstm_input_size = embedding_size + 32        else:            lstm_input_size = embedding_size        if embeddings is not None:            self.embedding = nn.Embedding(vocab_size, embedding_size). \                from_pretrained(embeddings, freeze=config.freeze_embedding)        self.num_layers = num_layers        if self.num_layers == 1:            dropout = 0.0        self.lstm = nn.LSTM(2 * hidden_size + lstm_input_size, hidden_size, dropout=dropout,                            num_layers=num_layers, bidirectional=True, batch_first=True)        self.ans_lstm = nn.LSTM(embedding_size, hidden_size, dropout=dropout, num_layers=1, bidirectional=True,                                batch_first=True)        self.key_lstm = nn.LSTM(embedding_size, hidden_size, dropout=dropout, num_layers=1, bidirectional=True,                                batch_first=True)        self.linear_trans = nn.Linear(2 * hidden_size, 2 * hidden_size)        self.update_layer = nn.Linear(4 * hidden_size, 2 * hidden_size, bias=False)        self.encoder_output_layer = nn.Linear(4 * hidden_size, 2 * hidden_size, bias=False)        self.gate = nn.Linear(4 * hidden_size, 2 * hidden_size, bias=False)        self.cross_gate = nn.Linear(2 * hidden_size, 2 * hidden_size, bias=False)        self.fusion_liner = nn.Linear(6 * hidden_size, 2 * hidden_size, bias=False)        self.gcn = GCN(input_dim=lstm_input_size, hidden_dim=hidden_size, num_layers=num_layers,                       dropout=dropout)        self.gcn_lstm_liner = nn.Linear(4 * hidden_size, 2 * hidden_size, bias=False)    def gated_self_attn(self, queries, memories, mask, debug=False):        # queries: [b,t,d]        # memories: [b,t,d]        # mask: [b,t]        energies = torch.matmul(queries, memories.transpose(1, 2))  # [b, t, t]        energies = energies.masked_fill(mask.unsqueeze(1), value=-1e12)        scores = F.softmax(energies, dim=2)        context = torch.matmul(scores, queries)        inputs = torch.cat([queries, context], dim=2)        f_t = torch.tanh(self.update_layer(inputs))        g_t = torch.sigmoid(self.gate(inputs))        updated_output = g_t * f_t + (1 - g_t) * queries        if debug:            print("energies", energies.size())            print("mask_ener", energies.size())            print("scores", scores.size())            print("context", context.size())            print("inputs", inputs.size())            print("ft", f_t.size())            print("gt", g_t.size())            print("update", updated_output.size())        return updated_output    def fill_context_mask(self, mask, sizes, v_mask, v_unmask):        mask.fill_(v_unmask)        n_context = mask.size(2)        for i, size in enumerate(sizes):            if size < n_context:                mask[i, :, size:] = v_mask        return mask    def dot(self, a, b):        return a.bmm(b.transpose(1, 2))    def attend(self, query, context, value=None, score='dot', normalize='softmax',               context_sizes=None, context_mask=None, return_weight=False, gated=False, debug=False               ):        q, c, v = query, context, value        if v is None:            v = c        batch_size_q, n_q, dim_q = q.size()        batch_size_c, n_c, dim_c = c.size()        batch_size_v, n_v, dim_v = v.size()        if not (batch_size_q == batch_size_c == batch_size_v):            msg = 'batch size mismatch (query: {}, context: {}, value: {})'            raise ValueError(msg.format(q.size(), c.size(), v.size()))        batch_size = batch_size_q        # Compute scores        if score == 'dot':            s = self.dot(q, c)            if debug:                print("s", s.size())        elif callable(score):            s = score(q, c)        else:            raise ValueError('unknown score function: {}'.format(score))        # Normalize scores and mask contexts        if normalize == 'softmax':            if context_mask is not None:                s = context_mask + s            elif context_sizes is not None:                context_mask = s.data.new(batch_size, n_q, n_c)                context_mask = self.fill_context_mask(context_mask,                                                      sizes=context_sizes,                                                      v_mask=float('-inf'),                                                      v_unmask=0                                                      )                s = context_mask + s            s_flat = s.view(batch_size * n_q, n_c)            if debug:                print("s_flat", s_flat.size())            w_flat = softmax(s_flat, dim=1)            if debug:                print("w_flat", w_flat.size())            w = w_flat.view(batch_size, n_q, n_c)            if debug:                print("w", w.size())        elif normalize == 'sigmoid' or normalize == 'identity':            w = sigmoid(s) if normalize == 'sigmoid' else s            if context_mask is not None:                w = context_mask * w            elif context_sizes is not None:                context_mask = s.data.new(batch_size, n_q, n_c)                context_mask = self.fill_context_mask(context_mask,                                                      sizes=context_sizes,                                                      v_mask=0,                                                      v_unmask=1                                                      )                w = context_mask * w        else:            raise ValueError('unknown normalize function: {}'.format(normalize))        # Combine        z = w.bmm(v)        if debug:            print("v", v.size())            print("z", z.size())        if gated:            f_t = torch.tanh(z)            if debug:                print("ft", f_t.size())            g_t = torch.sigmoid(self.cross_gate(z))            if debug:                print("gt", g_t.size())            z = g_t * f_t + (1 - g_t) * q            if debug:                print("update", z.size())        if return_weight:            return w, z        return z    def fusion(self, passage, answer, debug=False):        b, t, d = passage.size()        answer_zero = torch.zeros_like(passage)        answer_shape = answer.shape        if debug:            print("passage:", passage.size())            print("answer_zero:", answer_zero.size())            print("answer_shape", answer_shape)        answer_zero[:, :min(answer_shape[1], t), :] = answer[:, :min(answer_shape[1], t), :]        if debug:            # print(answer_zero)            print(answer_zero.size())        pass_ans_fuse = passage * answer_zero        fusion_outputs = self.fusion_liner(torch.cat([passage, answer_zero, pass_ans_fuse], dim=2))        if debug:            print(fusion_outputs.size())        return fusion_outputs    def forward(self, src_seq, src_len, tag_seq, ans_seq, ans_len, key_seq, key_len, adjs):        embedded = self.embedding(src_seq)        answer_embedded = self.embedding(ans_seq)        if config.use_tag and tag_seq is not None:            tag_embedded = self.tag_embedding(tag_seq)            embedded = torch.cat((embedded, tag_embedded), dim=2)        gcn_output, _ = self.gcn(adjs, embedded)        gcn_output = torch.cat([gcn_output[0], gcn_output[1]], dim=2)        enc_input = torch.cat([embedded, gcn_output], dim=2)        packed = pack_padded_sequence(enc_input, src_len, batch_first=True)        packed_ans = pack_padded_sequence(answer_embedded, ans_len, batch_first=True, enforce_sorted=False)        outputs, states = self.lstm(packed)  # states : tuple of [4, b, d]        ans_outputs, ans_states = self.ans_lstm(packed_ans)        outputs, _ = pad_packed_sequence(outputs, batch_first=True)  # [b, t, d]        ans_outputs, _ = pad_packed_sequence(ans_outputs, batch_first=True)        outputs = self.fusion(outputs, ans_outputs)        #h, c = states        h, c = ans_states        _, b, d = h.size()        h = h.view(1, 2, b, d)  # [n_layers, bi, b, d]        h = torch.cat((h[:, 0, :, :], h[:, 1, :, :]), dim=-1)        c = c.view(1, 2, b, d)        c = torch.cat((c[:, 0, :, :], c[:, 1, :, :]), dim=-1)        concat_states = (h, c)        return outputs, concat_statesclass Decoder(nn.Module):    def __init__(self, embeddings, vocab_size, embedding_size, hidden_size, num_layers, dropout):        super(Decoder, self).__init__()        self.vocab_size = vocab_size        self.embedding = nn.Embedding(vocab_size, embedding_size)        if embeddings is not None:            self.embedding = nn.Embedding(vocab_size, embedding_size). \                from_pretrained(embeddings, freeze=config.freeze_embedding)        if num_layers == 1:            dropout = 0.0        self.encoder_trans = nn.Linear(hidden_size, hidden_size)        self.reduce_layer = nn.Linear(embedding_size + hidden_size, embedding_size)        self.lstm = nn.LSTM(embedding_size, hidden_size, batch_first=True,                            num_layers=1, bidirectional=False, dropout=dropout)        self.concat_layer = nn.Linear(2 * hidden_size, hidden_size)        self.logit_layer = nn.Linear(hidden_size, vocab_size)    @staticmethod    def attention(query, memories, mask):        # query : [b, 1, d]        energy = torch.matmul(query, memories.transpose(1, 2))  # [b, 1, t]        energy = energy.squeeze(1).masked_fill(mask, value=-1e12)        attn_dist = F.softmax(energy, dim=1).unsqueeze(dim=1)  # [b, 1, t]        context_vector = torch.matmul(attn_dist, memories)  # [b, 1, d]        return context_vector, energy    def get_encoder_features(self, encoder_outputs):        return self.encoder_trans(encoder_outputs)    def forward(self, trg_seq, ext_src_seq, init_states, encoder_outputs, encoder_mask):        # trg_seq : [b,t]        # init_states : [2,b,d]        # encoder_outputs : [b,t,d]        # init_states : a tuple of [2, b, d]        batch_size, max_len = trg_seq.size()        hidden_size = encoder_outputs.size(-1)        # print(encoder_outputs.size())        memories = self.get_encoder_features(encoder_outputs)        logits = []        prev_states = init_states        prev_context = torch.zeros((batch_size, 1, hidden_size), device=config.device)        for i in range(max_len):            y_i = trg_seq[:, i].unsqueeze(1)  # [b, 1]            embedded = self.embedding(y_i)  # [b, 1, d]            lstm_inputs = self.reduce_layer(torch.cat([embedded, prev_context], dim=2))            output, states = self.lstm(lstm_inputs, prev_states)            # encoder-decoder attention            context, energy = self.attention(output, memories, encoder_mask)            concat_input = torch.cat((output, context), dim=2).squeeze(dim=1)            logit_input = torch.tanh(self.concat_layer(concat_input))            logit = self.logit_layer(logit_input)  # [b, |V|]            # maxout pointer network            if config.use_pointer:                num_oov = max(torch.max(ext_src_seq - self.vocab_size + 1), 0)                zeros = torch.zeros((batch_size, num_oov), device=config.device)                extended_logit = torch.cat([logit, zeros], dim=1)                out = torch.zeros_like(extended_logit) - INF                # print("out",out.size(),out)                # print("energy",energy.size(),energy)                # print("ext_src_seq",ext_src_seq.size(),ext_src_seq)                out, _ = scatter_max(energy, ext_src_seq, out=out)                # print("out_scatter",out.size(),out)                out = out.masked_fill(out == -INF, 0)                logit = extended_logit + out                logit = logit.masked_fill(logit == 0, -INF)            logits.append(logit)            # update prev state and context            prev_states = states            prev_context = context        logits = torch.stack(logits, dim=1)  # [b, t, |V|]        return logits    def decode(self, y, ext_x, prev_states, prev_context, encoder_features, encoder_mask):        # forward one step lstm        # y : [b]        embedded = self.embedding(y.unsqueeze(1))        lstm_inputs = self.reduce_layer(torch.cat([embedded, prev_context], dim=2))        output, states = self.lstm(lstm_inputs, prev_states)        context, energy = self.attention(output, encoder_features, encoder_mask)        concat_input = torch.cat((output, context), dim=2).squeeze(dim=1)        logit_input = torch.tanh(self.concat_layer(concat_input))        logit = self.logit_layer(logit_input)  # [b, |V|]        if config.use_pointer:            batch_size = y.size(0)            num_oov = max(torch.max(ext_x - self.vocab_size + 1), 0)            zeros = torch.zeros((batch_size, num_oov), device=config.device)            extended_logit = torch.cat([logit, zeros], dim=1)            out = torch.zeros_like(extended_logit) - INF            out, _ = scatter_max(energy, ext_x, out=out)            out = out.masked_fill(out == -INF, 0)            logit = extended_logit + out            logit = logit.masked_fill(logit == -INF, 0)            # forcing UNK prob 0            logit[:, UNK_ID] = -INF        return logit, states, contextclass Seq2seq(nn.Module):    def __init__(self, embedding=None, is_eval=False, model_path=None, is_test=False):        super(Seq2seq, self).__init__()        encoder = Encoder(embedding, config.vocab_size,                          config.embedding_size, config.hidden_size,                          config.num_layers,                          config.dropout)        decoder = Decoder(embedding, config.vocab_size,                          config.embedding_size, 2 * config.hidden_size,                          config.num_layers,                          config.dropout)        if config.use_gpu and torch.cuda.is_available():            device = torch.device(config.device)            encoder = encoder.to(device)            decoder = decoder.to(device)        self.encoder = encoder        self.decoder = decoder        if is_eval:            self.eval_mode()        if model_path is not None and is_test:            print("testing ......load model checkpoint form %s" % model_path)            ckpt = torch.load(model_path)            self.encoder.load_state_dict(ckpt["encoder_state_dict"])            self.decoder.load_state_dict(ckpt["decoder_state_dict"])        if model_path is not None and is_test is False:            print("pretraining ......load model checkpoint form %s" % model_path)            ckpt = torch.load(model_path)            encoder_dict = self.encoder.state_dict()            decoder_dict = self.decoder.state_dict()            filter = ['lstm.weight_ih_l0','lstm.weight_ih_l0_reverse','lstm.bias_ih_l0_reverse']            pretrained_encoder_dict = {k: v for k, v in ckpt["encoder_state_dict"].items() if                                       k in encoder_dict and k not in filter}            pretrained_decoder_dict = {k: v for k, v in ckpt["decoder_state_dict"].items() if                                       k in decoder_dict and k not in filter}            print("pre_trained_encoder_len:", len(pretrained_encoder_dict))            print("pre_trained_decoder_len:", len(pretrained_decoder_dict))            encoder_dict.update(pretrained_encoder_dict)            decoder_dict.update(pretrained_decoder_dict)            self.encoder.load_state_dict(encoder_dict)            self.decoder.load_state_dict(decoder_dict)    def eval_mode(self):        self.encoder = self.encoder.eval()        self.decoder = self.decoder.eval()    def train_mode(self):        self.encoder = self.encoder.train()        self.decoder = self.decoder.train()